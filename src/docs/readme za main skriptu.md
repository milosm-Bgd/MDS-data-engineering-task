
**main.py** docs

# main.py â€“ Orkestracija Data Engineering Pipeline-ova

Ovaj fajl predstavlja **ulaznu taÄku (entry point)** projekta i ima ulogu **orkestratora** koji pokreÄ‡e i koordinira dva nezavisna pipeline-a:

1. **Streaming pipeline** â€“ obrada poruka u realnom vremenu koriÅ¡Ä‡enjem minibatch pristupa
2. **File (batch) pipeline** â€“ obrada fajlova u noÄ‡nom / batch reÅ¾imu

Cilj dizajna je da demonstrira tipiÄne **data engineering obrasce**: producerâ€“consumer model, rad sa redovima (Queue), multithreading, metrike, graceful shutdown i modularnost.

---

## Pregled uvoza (imports)

```python
from queue import Queue
import threading
```

* `Queue` â€“ thread-safe red iz standardne biblioteke
* `threading` â€“ omoguÄ‡ava paralelno izvrÅ¡avanje worker niti

Ovo su osnovni gradivni blokovi za **producerâ€“consumer arhitekturu**.

---

### Streaming pipeline importi

```python
from common.config import MINIBATCH_WINDOW_SECONDS
from streaming.source import message_source
from streaming.minibatch import minibatch_collector
from streaming.retry import retry_with_backoff
from streaming.worker import worker
from streaming.processing import process_batch
```

Ovi moduli Äine **streaming podsistem**:

* `message_source` â€“ simulira real-time izvor poruka
* `minibatch_collector` â€“ grupiÅ¡e poruke u vremenske minibatch-e
* `worker` â€“ generiÄki worker koji obraÄ‘uje batch-e iz reda
* `process_batch` â€“ konkretna logika obrade jednog batch-a
* `retry_with_backoff` â€“ mehanizam za retry (koriÅ¡Ä‡en u worker-u)

---

### File pipeline importi

```python
from files.generator import create_fake_files
from files.bucketing import bfd_buckets
from files.processor import process_one_bucket
from files.worker import bucket_worker
from common.metrics import init_file_metrics
```

Ovi moduli implementiraju **batch / file pipeline**:

* `create_fake_files` â€“ generiÅ¡e laÅ¾ne fajlove za simulaciju
* `bfd_buckets` â€“ grupiÅ¡e fajlove u bucket-e po veliÄini
* `bucket_worker` â€“ worker za obradu bucket-a
* `process_one_bucket` â€“ logika obrade jednog bucket-a
* `init_file_metrics` â€“ inicijalizacija metrika za file pipeline

---

## Funkcija: `run_message_pipeline()`

Ova funkcija pokreÄ‡e **streaming pipeline**.

### Kreiranje queue-a /reda i osnovnih objekata

```python
work_queue = Queue(maxsize=50)
```

* Thread-safe red sa ograniÄenjem veliÄine
* ObezbeÄ‘uje **backpressure** ako producer proizvodi brÅ¾e nego Å¡to worker-i obraÄ‘uju

---

### Metrike i signal za zaustavljanje

```python
metrics = {
    "batches_ok": 0,
    "batches_failed": 0,
    "messages_processed": 0,
}
stop_event = threading.Event()
```

* `metrics` â€“ deljeni dictionary za praÄ‡enje uspeÅ¡nosti
* `stop_event` â€“ signal kojim se worker-i obaveÅ¡tavaju da treba da se zaustave

Ovo je standardan obrazac za **graceful shutdown** u multithreading okruÅ¾enju.

---

### Pokretanje worker niti

```python
for i in range(10):
    threading.Thread(
        target=worker,
        args=(i + 1, work_queue, metrics, stop_event, process_batch),
        daemon=True,
    ).start()
```

* PokreÄ‡e se 10 paralelnih worker-a
* Svaki worker:

  * Äita batch-e iz reda
  * obraÄ‘uje ih pomoÄ‡u `process_batch`
  * aÅ¾urira metrike

`daemon=True` znaÄi da se niti automatski gase kada se glavni proces zavrÅ¡i.

---

### Proizvodnja minibatch-eva

```python
minibatch_collector(
    message_source(duration_sec=300),
    window_sec=5,
    out_queue=work_queue,
)
```

* `message_source` emituje poruke tokom 5 minuta
* `minibatch_collector`:

  * skuplja poruke u vremenske prozore od 5 sekundi
  * formira minibatch-e
  * ubacuje ih u `work_queue`

Ovo simulira realan **streaming ingestion + windowing** scenario.

---

### ÄŒekanje zavrÅ¡etka obrade i gaÅ¡enje

```python
work_queue.join()
stop_event.set()
time.sleep(1)
print("STREAMING METRICS:", metrics)
```

* `queue.join()` blokira dok svi batch-evi ne budu obraÄ‘eni
* `stop_event.set()` signalizira worker-ima da izaÄ‘u iz petlje
* Na kraju se ispisuju metrike

---


## Funkcija: `run_file_pipeline()`

Ova funkcija implementira **batch / nightly file pipeline**.

### Generisanje fajlova i bucketing

```python
files = create_fake_files(100)
buckets = bfd_buckets(files, 10 * 1024 * 1024)
```

* GeneriÅ¡e se 100 laÅ¾nih fajlova
* Fajlovi se grupiÅ¡u u bucket-e maksimalne veliÄine 10 MB

Ovo simulira tipiÄan **ETL file ingestion** scenario.

---

### Punjenje reda

```python
queue = Queue(maxsize=50)
for b in buckets:
    queue.put(b)
```

* Svi bucket-i se stavljaju u red pre poÄetka obrade
* Red sluÅ¾i kao izvor posla za worker-e

---

### Worker-i i obrada

```python
metrics = init_file_metrics()
stop_event = threading.Event()

for i in range(10):
    threading.Thread(
        target=bucket_worker,
        args=(i + 1, queue, metrics, stop_event, process_one_bucket),
        daemon=True
    ).start()
```

* PokreÄ‡e se 10 worker-a
* Svaki worker obraÄ‘uje jedan bucket odjednom
* Metrike se centralno aÅ¾uriraju

---

### ZavrÅ¡etak pipeline-a

```python
queue.join()
stop_event.set()
print("NIGHTLY FILES METRICS:", metrics)
```

* ÄŒeka se da svi bucket-i budu obraÄ‘eni
* Signalizira se zaustavljanje worker-a
* Ispisuju se rezultati

---

## CLI interfejs (`__main__` blok)

```python
parser.add_argument(
    "--pipeline",
    choices=["files", "streaming", "all"],
    default="files",
)
```

OmoguÄ‡ava pokretanje pipeline-a iz komandne linije:

* `files` â€“ samo file pipeline
* `streaming` â€“ samo streaming pipeline
* `all` â€“ oba pipeline-a paralelno

---

### Paralelno pokretanje oba pipeline-a

```python
t = threading.Thread(target=run_message_pipeline)
t.start()

run_file_pipeline()
t.join()
```

* Streaming pipeline se pokreÄ‡e u posebnoj niti
* File pipeline se izvrÅ¡ava u glavnoj niti
* Na kraju se Äeka zavrÅ¡etak oba

Ovo demonstrira **koegzistenciju real-time i batch sistema**, Å¡to je Äest sluÄaj u realnim data platformama.

---

## ZakljuÄak

`main.py` predstavlja centralni orkestracioni sloj koji:

* jasno razdvaja streaming i batch logiku
* koristi standardne concurrency obrasce
* omoguÄ‡ava testiranje, proÅ¡irenje i demonstraciju znanja iz oblasti data engineering-a

Ovaj fajl je posebno pogodan za **intervjue i code review**, jer pokazuje razumevanje:

* multithreading-a
* producerâ€“consumer arhitekture
* backpressure mehanizama
* metrika i graceful shutdown-a



Dokumentacija za **`main.py`** je sada kreirana na srpskom jeziku i prikazana u **canvasu** pored chata.

ğŸ“Œ Å ta ova dokumentacija pokriva (Å¡to je vaÅ¾no za reviewere):

* jasnu **ulogu fajla kao orkestratora**
* razdvajanje **streaming** i **batch (file)** pipeline-ova
* objaÅ¡njenje **threading modela**, `Queue`-a i `Event`-a
* obrazloÅ¾enje dizajnerskih odluka (backpressure, graceful shutdown)
* CLI interfejs i paralelno izvrÅ¡avanje pipeline-ova
* terminologiju koja se poklapa sa realnim **data engineering sistemima**

ğŸ“‚ Ovaj tekst je spreman za:

* `README` podsekciju
* zaseban `docs/main.md`
* direktan code review u javnom GitHub repozitorijumu

ğŸ‘‰ SledeÄ‡i logiÄan korak u seriji:

* `streaming/source.py` (message_source)
* ili `streaming/minibatch.py`
* ili `worker` / `retry_with_backoff` (odliÄni za intervju)
